# Projet : Construction d'un Pipeline ETL avec PySpark pour l'analyse des trajets en Taxi

## Description

Le secteur des transports, en particulier les services de taxi, constitue un √©l√©ment crucial de l'infrastructure urbaine de New York. Avec des millions de voyages effectu√©s chaque ann√©e, les donn√©es g√©n√©r√©es par ces voyages fournissent une mine d'informations pr√©cieuses. En tant que Data Scientist nouvellement embauch√© dans une soci√©t√© d'analyse des transport, votre Manager vous demande de collecter des donn√©es sur les trajets en taxi de la ville de New York √† partir de la source officielle fournie par la *NYC Taxi and Limousine Commission (TLC)*. Une fois les donn√©es collect√©s, vous devez construire un pipeline de donn√©es qui permettra d'ex√©cuter un processus ETL (Extract, Tranform, Load) dans le but ultime de charger les donn√©es nettoy√©es/tranform√©es dans un fichier PARQUET afin des les utliser pour de futures analyses.

## Image
imgs/project6/project6.png

## Instructions

1. **T√©l√©chargement des fichiers de donn√©es sur les trajets en taxi jaune (Yellow Taxi Trip Records)** : Ces fichiers sont actuellement disponibles au format PARQUET sur le site de la [NYC Taxi and Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Vous devez √©crire un script python *download_data_files.py* qui permettra d'effectuer automatiquement ce t√©l√©chargement sur la p√©riode d'√©tude souhait√©e (par exemple Janvier 2021 √† F√©vrier 2024). Cela d√©pendra aussi de la puissance de votre machine. Vous pouvez choisir une p√©riode beaucoup plus courte si n√©cessaire. L'ex√©cution de ce script aura pour sortie un dossier nomm√© "histo_data_files" qui contiendra tous les fichiers PARQUET t√©l√©charg√©s.

2. **Ecriture du Script Python qui contiendra les fonctions d'extraction, de transformation et de chargement des donn√©es** : Ce script nomm√© *etl_functions.py* devra contenir 3 fonctions : 

    - extract : pour extraire les donn√©es d√∫n fichier PARQUET et retourner une dataframe PySpark

    - transform : pour filtrer une dataframe PySpark en supprimant les observations avec des valeurs manquantes ou pour lesquelles trip_distance, passager_count ou total_amount est inf√©rieur ou √©gal √† 0 ; Fusionner la DataFrame filtr√©e avec les informations de [zone](https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv) ; Concatener les informations de l'arrondissement pour les lieux de prise en charge (variable "puborough") et de d√©p√¥t (variable "doborough") ; supprimer les valeurs manquantes et retourner la dataframe ainsi transform√©e

    - load : pour sauvegarder une dataframe PySPark au format PARQUET

Les codes de toutes les fonctions de ce projet doivent √™tre √©crites dans des blocs try ... except en y int√©grand des messages de logging pour faciliter le d√©bogage en cas de probl√®me. Cela fait partie des bonnes pratiques de Software Engineering.

3. **Ecriture du Script Python qui permettra d'ex√©cuter le processus ETL** : Ce script *etl_pipeline.py* fera appel au module *etl_functions.py* et son ex√©cution aura comme r√©sultat un fichier PARQUET des donn√©es transform√©es : "data_loaded/transformed_taxi_data.parquet".


## Resources

- [Source des donn√©es](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

- [Donn√©es sur les zones de prise en charge et de d√©pose](https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv)

- [Comment automatiser le t√©l√©chargement des fichiers de donn√©es Big Data avec Python ?](https://youtu.be/6Mzmg4E78R0)

- [Tutoriel sur PySpark](https://youtu.be/QCuQzktfQV4)

- [ùêèùê≤ùêíùê©ùêöùê´ùê§ ùêûùêß ùê©ùê´ùêöùê≠ùê¢ùê™ùêÆùêû : ùêÇùêöùê¨ ùêù'ùêÆùê¨ùêöùê†ùêûùê¨ ùê´ùêûÃÅùêûùê•ùê¨ ùêûùê≠ ùêûùê±ùêûùê¶ùê©ùê•ùêûùê¨ ùê©ùê´ùêöùê≠ùê¢ùê™ùêÆùêûùê¨ ùêûùêß ùêÉùêöùê≠ùêö ùêíùêúùê¢ùêûùêßùêúùêû ùêûùê≠ ùêåùêöùêúùê°ùê¢ùêßùêû ùêãùêûùêöùê´ùêßùê¢ùêßùê† (Version Papier)](https://www.amazon.fr/gp/product/B0C9K6GTNH/ref=dbs_a_def_rwt_hsch_vamf_tkin_p1_i7)

- [ùêèùê≤ùêíùê©ùêöùê´ùê§ ùêûùêß ùê©ùê´ùêöùê≠ùê¢ùê™ùêÆùêû : ùêÇùêöùê¨ ùêù'ùêÆùê¨ùêöùê†ùêûùê¨ ùê´ùêûÃÅùêûùê•ùê¨ ùêûùê≠ ùêûùê±ùêûùê¶ùê©ùê•ùêûùê¨ ùê©ùê´ùêöùê≠ùê¢ùê™ùêÆùêûùê¨ ùêûùêß ùêÉùêöùê≠ùêö ùêíùêúùê¢ùêûùêßùêúùêû ùêûùê≠ ùêåùêöùêúùê°ùê¢ùêßùêû ùêãùêûùêöùê´ùêßùê¢ùêßùê† (Version PDF)]( https://afoudajosue.gumroad.com/l/yeatg)

- [Documentation sur l'installation de Spark](https://spark.apache.org/downloads.html)

- [Installation et Configuration d'un environnement Python avec VSC](https://youtu.be/6NYsMiFqH3E)


## Execution du Projet

üñ• **T√©l√©chargement des donn√©es historiques : Script download_data_files.py**

```python
import os
import requests
import time
import hashlib
from datetime import datetime

def download_histo_data(path_to_histo_data_folder):
    """
    Downloads yellow taxi trip PARQUET files from 2021 to current year into the specified folder.

    Parameters:
    path_to_histo_data_folder (str): Path to the folder where files will be saved.

    Returns:
    None
    """
    try:
        # Create a folder to store downloaded files if it doesn't exist
        os.makedirs(path_to_histo_data_folder, exist_ok=True)

        # Current year
        current_year = datetime.now().year

        # Loop over years from current year to 2021
        for year in range(current_year, 2020, -1):
            for month in range(1, 13):
                # Construct download URL based on year and month
                download_url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet"
                file_name = f"{path_to_histo_data_folder}/yellow_tripdata_{year}-{month:02d}.parquet"

                # Check if the file already exists
                if os.path.exists(file_name):
                    print(f"The file {file_name} already exists, skipping to the next file...")
                    continue

                # Download the file with error handling
                try:
                    print(f"Downloading {file_name}...")
                    response = requests.get(download_url, stream=True)
                    if response.status_code == 200:
                        with open(file_name, "wb") as f:
                            for chunk in response.iter_content(chunk_size=1024):
                                f.write(chunk)
                        print(f"{file_name} downloaded successfully!")
                    else:
                        print(f"Failed to download {file_name}. HTTP status code: {response.status_code}")
                except Exception as e:
                    print(f"An error occurred while downloading {file_name}: {str(e)}")

                # Pause for 1 second between each download to avoid overloading the remote server
                time.sleep(1)

    except Exception as e:
        print(f"An error occurred: {str(e)}")

    print("Download completed!")

# Date of the day
print("Date of historical data download:", datetime.today())

# Path to the folder to save historical data
path_to_histo_data_folder = "histo_data_files"

# Call the function to download historical data
download_histo_data(path_to_histo_data_folder)
```

Le script `download_data_files.py` est con√ßu pour t√©l√©charger des fichiers de donn√©es de trajets en taxi jaune au format PARQUET depuis un serveur distant pour chaque mois de l'ann√©e, de 2021 jusqu'√† l'ann√©e en cours, et les sauvegarder dans un dossier sp√©cifi√©. Voici une explication d√©taill√©e de chaque partie du script :

- Importation des biblioth√®ques n√©cessaires

```python
import os
import requests
import time
from datetime import datetime
```

- `os` : pour des op√©rations sur le syst√®me de fichiers, comme cr√©er des dossiers ou v√©rifier l'existence de fichiers.
- `requests` : pour faire des requ√™tes HTTP et t√©l√©charger des fichiers.
- `time` : pour introduire des pauses dans le script.
- `datetime` : pour travailler avec les dates et les heures.

- D√©finition de la fonction principale

```python
def download_histo_data(path_to_histo_data_folder):
```

Cette fonction, `download_histo_data`, t√©l√©charge les fichiers de donn√©es historiques de trajets en taxi jaune et les enregistre dans un dossier sp√©cifi√© par le param√®tre `path_to_histo_data_folder`.

- Cr√©ation du dossier de destination

```python
    try:
        os.makedirs(path_to_histo_data_folder, exist_ok=True)
```

- `os.makedirs` : cr√©e un dossier, ainsi que tous les dossiers parents n√©cessaires. Le param√®tre `exist_ok=True` permet d'√©viter une erreur si le dossier existe d√©j√†.

- D√©termination de l'ann√©e en cours

```python
        current_year = datetime.now().year
```

- `datetime.now().year` : obtient l'ann√©e actuelle.

- Boucle sur les ann√©es et les mois

```python
        for year in range(current_year, 2020, -1):
            for month in range(1, 13):
```

- `range(current_year, 2020, -1)` : cr√©e une plage d'ann√©es allant de l'ann√©e en cours √† 2021 (inclus).
- `range(1, 13)` : cr√©e une plage de mois de janvier (1) √† d√©cembre (12).

- Construction de l'URL et du nom de fichier

```python
                download_url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet"
                file_name = f"{path_to_histo_data_folder}/yellow_tripdata_{year}-{month:02d}.parquet"
```

- `f"{year}-{month:02d}"` : formatte l'ann√©e et le mois avec deux chiffres pour le mois (par exemple, `2021-01`).

- V√©rification de l'existence du fichier

```python
                if os.path.exists(file_name):
                    print(f"The file {file_name} already exists, skipping to the next file...")
                    continue
```

- `os.path.exists(file_name)` : v√©rifie si le fichier existe d√©j√† pour √©viter de le t√©l√©charger √† nouveau.

- T√©l√©chargement du fichier

```python
                try:
                    print(f"Downloading {file_name}...")
                    response = requests.get(download_url, stream=True)
                    if response.status_code == 200:
                        with open(file_name, "wb") as f:
                            for chunk in response.iter_content(chunk_size=1024):
                                f.write(chunk)
                        print(f"{file_name} downloaded successfully!")
                    else:
                        print(f"Failed to download {file_name}. HTTP status code: {response.status_code}")
                except Exception as e:
                    print(f"An error occurred while downloading {file_name}: {str(e)}")
```

- `requests.get(download_url, stream=True)` : fait une requ√™te HTTP pour t√©l√©charger le fichier. Le param√®tre `stream=True` permet de t√©l√©charger le fichier par petits morceaux (chunks).
- `response.status_code` : v√©rifie si la requ√™te a r√©ussi (code 200).
- `with open(file_name, "wb") as f` : ouvre le fichier en mode binaire pour √©crire (`"wb"`).
- `response.iter_content(chunk_size=1024)` : lit le contenu de la r√©ponse par morceaux de 1024 octets pour √©conomiser de la m√©moire.
- `f.write(chunk)` : √©crit chaque morceau dans le fichier.

- Pause entre les t√©l√©chargements

```python
                time.sleep(1)
```

- `time.sleep(1)` : attend 1 seconde entre chaque t√©l√©chargement pour √©viter de surcharger le serveur.

- Gestion des erreurs globales

```python
    except Exception as e:
        print(f"An error occurred: {str(e)}")

    print("Download completed!")
```

- `except Exception as e` : capture toute exception non pr√©vue pendant l'ex√©cution du code.
- `print(f"An error occurred: {str(e)}")` : affiche un message d'erreur.

- Ex√©cution du script

```python
# Date of the day
print("Date of historical data download:", datetime.today())

# Path to the folder to save historical data
path_to_histo_data_folder = "histo_data_files"

# Call the function to download historical data
download_histo_data(path_to_histo_data_folder)
```

- `print("Date of historical data download:", datetime.today())` : affiche la date actuelle.
- `path_to_histo_data_folder = "histo_data_files"` : d√©finit le chemin du dossier o√π les fichiers seront sauvegard√©s.
- `download_histo_data(path_to_histo_data_folder)` : appelle la fonction pour t√©l√©charger les donn√©es.

Ce script permet de s'assurer que toutes les donn√©es historiques de trajets en taxi jaune sont t√©l√©charg√©es et sauvegard√©es localement, tout en g√©rant les erreurs et en √©vitant de t√©l√©charger plusieurs fois les m√™mes fichiers.

üñ• **Fonctions du processus ETL : Script etl_functions.py**

```python
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws
from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, LongType, DoubleType, StringType
from pyspark.sql import DataFrame
import pandas as pd

# Create a SparkSession
spark = SparkSession.builder \
    .appName("ETL Pipeline") \
    .getOrCreate()

# https://stackoverflow.com/questions/59096125/spark-2-4-parquet-column-cannot-be-converted-in-file-column-impressions-exp
spark.conf.set("spark.sql.parquet.enableVectorizedReader","false")

# Ajuster la configuration pour la repr√©sentation en cha√Æne des plans d'ex√©cution
spark.conf.set("spark.sql.debug.maxToStringFields", "255")

def extract(file_path):
    """
    Extracts data from a Parquet file and returns a PySpark DataFrame.

    Parameters:
    file_path (str): The path to the Parquet file.

    Returns:
    pyspark.sql.DataFrame: DataFrame containing the data from the Parquet file.
    """
    try:
        # Initialize logging
        logging.basicConfig(filename='extract.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # Reading Parquet file into DataFrame
        df = spark.read.parquet(file_path)

        logging.info("Parquet file extracted successfully.")

        return df
    except Exception as e:
        logging.error("An error occurred while extracting Parquet file: %s", str(e))
        return None


def transform(df):
    """
    Filters the DataFrame by removing observations with missing values or where trip_distance, passenger_count, or total_amount is less than or equal to 0.
    Merges the filtered DataFrame with zone information.
    Concatenates borough information for pickup and dropoff locations.
    
    Parameters:
    df (pyspark.sql.DataFrame): Input DataFrame.

    Returns:
    pyspark.sql.DataFrame: Transformed DataFrame.
    """
    try:
        # Initialize logging
        logging.basicConfig(filename='transform.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # Renaming columns to lowercase
        for column_name in df.columns:
            df = df.withColumnRenamed(column_name, column_name.lower())

        # Dropping rows with missing values and filtering by specified conditions
        filtered_df = df.na.drop(subset=["passenger_count", "total_amount"]) \
            .filter((col("trip_distance") > 0) & (col("passenger_count") > 0) & (col("total_amount") > 0))

        # Read zone information 
        df_zones = pd.read_csv("fichier_des_zones.csv")

        # Select relevant columns from df_zones
        df_zones_subset = df_zones[['locationid', 'borough', 'zone', 'service_zone']]

        # Convert df_zones-subset to a Spark dataframe
        df_zones_subset = spark.createDataFrame(df_zones_subset)

        # Merge filtered DataFrame with zone information for pickup locations
        merged = filtered_df.join(df_zones_subset,
                           on=filtered_df.pulocationid == df_zones_subset.locationid,
                           how="left")
        
        # Renommage des colonnes
        merged = merged.withColumnRenamed("borough", "puborough") \
                                 .withColumnRenamed("zone", "puzone") \
                                 .withColumnRenamed("service_zone", "pu_service_zone") #\
                                 #.withColumnRenamed("locationid", "pulocationid")

        merged = merged.drop("locationid")

        # Merge filtered DataFrame with zone information for dropoff locations
        merged = merged.join(df_zones_subset,
                     on=merged.dolocationid == df_zones_subset.locationid,
                     how="left")
        
        # Renommage des colonnes
        merged = merged.withColumnRenamed("borough", "doborough") \
                                 .withColumnRenamed("zone", "dozone") \
                                 .withColumnRenamed("service_zone", "do_service_zone") #\
                                 #.withColumnRenamed("locationid", "dolocationid")
        
        merged = merged.drop("locationid")

        # Concatenate borough information for pickup and dropoff locations
        merged = merged.withColumn("Itineraire Arrondissement", concat_ws(" - ", merged["puborough"], merged["doborough"]))
        merged = merged.withColumn("Itineraire zone", concat_ws(" - ", merged["puzone"], merged["dozone"]))

        # Drop rows with null values
        merged = merged.dropna()

        logging.info("Data transformation completed successfully.")

        return merged
    
    except Exception as e:
        logging.error("An error occurred while transforming the data: %s", str(e))
        return None


def load(df: DataFrame, file_path: str):
    """
    Saves a PySpark DataFrame to Parquet format.

    Parameters:
    df (pyspark.sql.DataFrame): Input PySpark DataFrame.
    file_path (str): The path where the Parquet file will be saved.

    Returns:
    None
    """
    try:
        # Set up logging
        logging.basicConfig(filename='load.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # Save DataFrame to Parquet
        df.write.parquet(file_path, mode="append")

        logging.info("Data loaded successfully into Parquet.")

    except Exception as e:
        logging.error("An error occurred while saving DataFrame to Parquet: %s", str(e))
```


Le script `etl_functions.py` d√©finit trois fonctions principales pour un pipeline ETL (Extract, Transform, Load) en utilisant PySpark et SQLAlchemy. Voici une explication d√©taill√©e de chaque section du script et des fonctions d√©finies :

- Importation des biblioth√®ques n√©cessaires

```python
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws
from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, LongType, DoubleType, StringType
from pyspark.sql import DataFrame
import pandas as pd
```

- `logging` : pour la journalisation des √©v√©nements du script.
- `pyspark.sql` : pour manipuler les DataFrames avec PySpark.
- `pandas` : pour manipuler les donn√©es tabulaires en Python.


- Cr√©ation de la session Spark

```python
# Create a SparkSession
spark = SparkSession.builder \
    .appName("ETL Pipeline") \
    .getOrCreate()

# https://stackoverflow.com/questions/59096125/spark-2-4-parquet-column-cannot-be-converted-in-file-column-impressions-exp
spark.conf.set("spark.sql.parquet.enableVectorizedReader","false") 

# Ajuster la configuration pour la repr√©sentation en cha√Æne des plans d'ex√©cution
spark.conf.set("spark.sql.debug.maxToStringFields", "255")
```

- Initialisation d'une session Spark.
- Configuration de Spark pour d√©sactiver le lecteur vectoris√© pour les fichiers Parquet et ajuster la limite de champs pour la repr√©sentation en cha√Æne.

- Fonction d'extraction des donn√©es

- **Param√®tre** : `file_path` - le chemin du fichier Parquet √† extraire.
- **Retour** : un DataFrame PySpark contenant les donn√©es extraites.
- **Fonctionnement** :
  - Configure la journalisation.
  - Lit le fichier Parquet et le charge dans un DataFrame PySpark.
  - Journalise le succ√®s ou l'√©chec de l'op√©ration.

- Fonction de transformation des donn√©es

- **Param√®tre** : `df` - le DataFrame PySpark √† transformer.
- **Retour** : un DataFrame PySpark transform√©.
- **Fonctionnement** :
  - Configure la journalisation.
  - Renomme les colonnes en minuscules.
  - Supprime les lignes avec des valeurs manquantes et filtre les donn√©es selon certaines conditions.
  - Lit les informations de zones √† partir d√∫n fichier CSV
  - Fusionne le DataFrame filtr√© avec les informations de zones pour les lieux de prise en charge et de d√©pose.
  - Renomme les colonnes r√©sultantes pour les rendre plus explicites.
  - Concat√®ne les informations des arrondissements et des zones.
  - Supprime les lignes avec des valeurs nulles.


- Fonction de chargement des donn√©es

- **Param√®tres** :
  - `df` - le DataFrame PySpark √† sauvegarder.
  - `file_path` - le chemin o√π le fichier Parquet sera sauvegard√©.
- **Retour** : Aucun.
- **Fonctionnement** :
  - Configure la journalisation.
  - Sauvegarde le DataFrame en format Parquet avec le mode "append" pour ajouter les donn√©es sans √©craser les fichiers existants.
  - Journalise le succ√®s ou l'√©chec de l'op√©ration.

- Conclusion

Le script `etl_functions.py` configure une session Spark puis d√©finit trois fonctions pour extraire, transformer et charger des donn√©es. Ces fonctions utilisent la journalisation pour enregistrer les succ√®s et les erreurs, ce qui facilite le suivi et le d√©bogage du processus ETL.


üñ• **Ex√©cution du processus ETL : Script etl_pipeline.py**

```python
import etl_functions
import logging
import os

# Configuration des logs
logging.basicConfig(filename='pyspark_etl_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define file paths
#input_file_path = "histo_data_files"  # Adjust the path to your Parquet files
output_folder = "data_loaded"  # Dossier pour enregistrer les donn√©es tranformees

# Create output folder if it doesn't exist
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# Liste des fichiers Parquet dans le dossier
parquet_files = [f for f in os.listdir("histo_data_files") if f.endswith('.parquet')]

# Pour chaque fichier Parquet
for file_name in parquet_files:
    # Extract data
    logging.info("Starting data extraction...")
    input_df = etl_functions.extract("histo_data_files/" + file_name)
    logging.info("Data extraction completed.")

    # Transform data
    logging.info("Starting data transformation...")
    transformed_df = etl_functions.transform(input_df)
    logging.info("Data transformation completed.")

    # Load data
    if transformed_df is not None:
        logging.info("Starting data loading...")
        output_file_path = output_folder + "/transformed_taxi_data.parquet"
        etl_functions.load(transformed_df, output_file_path)
        logging.info("Data loading completed.")
```


Le script `etl_pipeline.py` est con√ßu pour orchestrer les √©tapes d'un pipeline ETL (Extract, Transform, Load) en utilisant les fonctions d√©finies dans `etl_functions.py`. Voici une explication d√©taill√©e de chaque section du script :

- Importation des biblioth√®ques et modules n√©cessaires

```python
import etl_functions
import logging
import os
```

- `etl_functions` : importation des fonctions ETL d√©finies dans un autre script (`etl_functions.py`).
- `logging` : pour la journalisation des √©v√©nements du script.
- `os` : pour les op√©rations sur le syst√®me de fichiers, comme v√©rifier l'existence de dossiers ou lister des fichiers.

- Configuration de la journalisation

```python
logging.basicConfig(filename='pyspark_etl_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
```

- Configure la journalisation pour enregistrer les messages dans un fichier `pyspark_etl_pipeline.log` avec le niveau `INFO` et un format de message sp√©cifiant l'heure, le niveau de log et le message.

- D√©finition des chemins de fichiers

```python
# Define file paths
#input_file_path = "histo_data_files"  # Adjust the path to your Parquet files
output_folder = "data_loaded"  # Dossier pour enregistrer les donn√©es transform√©es
```

- `output_folder` : sp√©cifie le dossier o√π les donn√©es transform√©es seront enregistr√©es.

- Cr√©ation du dossier de sortie s'il n'existe pas

```python
# Create output folder if it doesn't exist
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
```

- V√©rifie si le dossier de sortie existe et le cr√©e s'il n'existe pas.

- Liste des fichiers Parquet dans le dossier d'entr√©e

```python
# Liste des fichiers Parquet dans le dossier
parquet_files = [f for f in os.listdir("histo_data_files") if f.endswith('.parquet')]
```

- Liste tous les fichiers dans le dossier `histo_data_files` qui se terminent par `.parquet` et les stocke dans `parquet_files`.

- Boucle sur chaque fichier Parquet pour les √©tapes ETL

```python
# Pour chaque fichier Parquet
for file_name in parquet_files:
    # Extract data
    logging.info("Starting data extraction...")
    input_df = etl_functions.extract("histo_data_files/" + file_name)
    logging.info("Data extraction completed.")

    # Transform data
    logging.info("Starting data transformation...")
    transformed_df = etl_functions.transform(input_df)
    logging.info("Data transformation completed.")

    # Load data
    if transformed_df is not None:
        logging.info("Starting data loading...")
        output_file_path = output_folder + "/transformed_taxi_data.parquet"
        etl_functions.load(transformed_df, output_file_path)
        logging.info("Data loading completed.")
```

- Pour chaque fichier Parquet dans `parquet_files` :
  1. **Extraction des donn√©es** :
     - Journalise le d√©but de l'extraction.
     - Appelle la fonction `extract` du module `etl_functions` pour lire le fichier Parquet et stocke le DataFrame r√©sultant dans `input_df`.
     - Journalise la fin de l'extraction.
  2. **Transformation des donn√©es** :
     - Journalise le d√©but de la transformation.
     - Appelle la fonction `transform` du module `etl_functions` pour transformer le DataFrame `input_df` et stocke le DataFrame transform√© dans `transformed_df`.
     - Journalise la fin de la transformation.
  3. **Chargement des donn√©es** :
     - Si `transformed_df` n'est pas `None` (c'est-√†-dire si la transformation a r√©ussi), journalise le d√©but du chargement.
     - D√©finit le chemin du fichier de sortie (`output_file_path`).
     - Appelle la fonction `load` du module `etl_functions` pour sauvegarder le DataFrame transform√© dans un fichier Parquet.
     - Journalise la fin du chargement.

- Conclusion

Le script `etl_pipeline.py` est un orchestrateur simple et efficace pour ex√©cuter les √©tapes d'extraction, de transformation et de chargement des donn√©es sur une s√©rie de fichiers Parquet. Chaque √©tape est journalis√©e, permettant un suivi facile des processus et un d√©bogage en cas de probl√®me. Les fonctions sp√©cifiques √† chaque √©tape (extraction, transformation, chargement) sont import√©es du script `etl_functions.py`, rendant le code modulaire et facile √† maintenir.


Pour ex√©cuter ce pipeline ETL, vous devez ex√©cuter ces commandes dans l'ordre :

1. T√©l√©chargement des donn√©es brutes

```bash
python3 download_data_files.py
``` 


2. Ex√©cution du Pipeline

```bash
python3 etl_pipeline.py
``` 

Les principaux packages requis pour ce projet sont :

numpy==1.26.4

pandas==2.2.2

pyarrow==16.0.0

requests==2.31.0

pyspark==3.5.1